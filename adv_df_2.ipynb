{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9c40bd-a38f-4691-ba02-4db792a56d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0a292-bef9-4a0c-a9c1-7bdfc87f9734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1349)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6918c96f-ad1a-4b7e-9bee-c2c48bc7f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of values for names column.\n",
    "\n",
    "students = ['Sally', 'Jane', 'Suzie', 'Billy', 'Ada', 'John', 'Thomas',\n",
    "            'Marie', 'Albert', 'Richard', 'Isaac', 'Alan']\n",
    "\n",
    "# Randomly generate arrays of scores for each student for each subject.\n",
    "# Note that all the values need to have the same length here.\n",
    "\n",
    "math_grades = np.random.randint(low=60, high=100, size=len(students))\n",
    "english_grades = np.random.randint(low=60, high=100, size=len(students))\n",
    "reading_grades = np.random.randint(low=60, high=100, size=len(students))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f17639-cf51-4ea1-8103-a2f87f2214aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'name': students,\n",
    "                   'math': math_grades,\n",
    "                   'english': english_grades,\n",
    "                   'reading': reading_grades,\n",
    "                   'classroom': np.random.choice(['A', 'B'], len(students))})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5149c5-9f84-457a-b244-b8cf32d57508",
   "metadata": {},
   "source": [
    "## Indexing and Subsetting\n",
    "\n",
    "Like the pandas Series object, the pandas DataFrame object supports both position- and label-based indexing using the indexing operator `[]`.\n",
    "\n",
    "I will demonstrate concrete examples of indexing using the indexing operator `[]` alone and with the `.loc` and `.iloc` attributes below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3950721c-81e4-4b99-85b5-82938dcc5a23",
   "metadata": {},
   "source": [
    "### `[]`\n",
    "\n",
    "I can pass a list of columns from a DataFrame to the indexing operator (aka bracket notation) to return a subset of my original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e8f02-2ee3-406a-972d-26847fea261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5488d9d-58bd-4a20-86ab-aea2f6beaf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can choose a single column using bracket notation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6bf6c6-3e14-4361-a9fc-7f8f7cfc2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose only two columns for my subset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468e79b1-a930-42a4-b06d-3e1a7c36da68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can pass a boolean Series to the indexing operator as a selector.\n",
    "# names that start with a?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6dc48-6f1f-4ccc-8d15-7481bb3a1a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[bools]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d63c3-51f3-42a6-9bb5-3649ee50bab9",
   "metadata": {},
   "source": [
    "### `.loc`\n",
    "\n",
    "We can use the `.loc` attribute to select specific rows AND columns by index label. The index label can be a number, but it can also be a string label. This method offers a lot of flexibility! **The .loc attribute's indexing is inclusive and uses an index label, not position.**\n",
    "\n",
    "```python\n",
    "df.loc[row_indexer, column_indexer]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c788e33-97cd-4ab4-920f-76fc2bc34fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9573e1-8e10-4c37-9c0a-3637823383d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a2761-b351-448a-a1f6-ccd35c7451a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all the rows and a subset of columns; notice the inclusive behavior of the indexing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0d46ae-e42a-4419-99c1-1a0b8cd63866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can use a boolean Series as a selector with .loc, too, but I can choose rows and columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed6e1e-56bd-4306-be48-ac0d3cdc8597",
   "metadata": {},
   "source": [
    "### `.iloc`\n",
    "\n",
    "We can use the `.iloc` attribute to select specific rows and colums by index position. `.iloc` does not accept a boolean Series as a selector like `.loc` does. **It takes in integers representing index position and is NOT inclusive.**\n",
    "\n",
    "```python\n",
    "df.iloc[row_indexer, column_indexer]\n",
    "```\n",
    "\n",
    "We can select rows by integer position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad78ff9-f255-408b-a544-62856eb82bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the exclusive behavior of the indexing.\n",
    "\n",
    "df.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f9e1e7-2f8a-410a-baf1-a377e3da59b4",
   "metadata": {},
   "source": [
    "We can also specify which columns we want to select:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb356cb6-b150-4908-96fd-c066880b4208",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:3, 1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b679a3cb-0a1d-4286-b8b4-c6e1ed6557d0",
   "metadata": {},
   "source": [
    "Here we select the first 3 rows (everything up to but not including the index of 3), and the second and third columns (starting from the index of 1 up to but not including the index of 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11b05bd-14d6-4b7d-baf6-9db2a14611d9",
   "metadata": {},
   "source": [
    "## Aggregating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766819b-d9ab-4779-93d3-ebc9277d22ed",
   "metadata": {},
   "source": [
    "### `.agg`\n",
    "\n",
    "The `.agg` method lets us specify a way to aggregate a series of numerical values. We pass an aggregate function or list of functions to the method that we want applied to a Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128069b5-85f8-4031-b5bd-bc348f7d5013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can use it on the entire df.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985578ff-f3cc-45d8-b32b-c363d1f64440",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91966ace-f998-435e-b42a-dcee2906d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can use it on a single column in a df.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23b9b67-0e77-462e-a843-4788c22d9ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can pass a list of functions to the .agg method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f2c878-afd5-4799-a66c-581e5e323f17",
   "metadata": {},
   "source": [
    "While on the surface this seems pretty simple, `.agg` is capable of providing more detailed aggregations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd55f0bd-a2a0-4b4c-ae42-e0da1d5bd69a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dba85bfe-1480-4f62-904b-864656abe931",
   "metadata": {},
   "source": [
    "### `.groupby`\n",
    "\n",
    "The `.groupby` method is used to create a grouped object, which we can then apply an aggregation on. For example, if we wanted to know the highest math grade from each classroom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ae2ef1-3f7a-422f-8302-dccfba9190c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('classroom').math.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8120f47-cc79-44e2-91f7-fdeed316292b",
   "metadata": {},
   "source": [
    "We can use `.agg` here to, to see multiple aggregations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3331b889-0be6-4cdb-b454-1174159ce553",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby1 = df.groupby('classroom').math.agg(['min', 'mean', 'max'])\n",
    "groupby1.columns = ['math_min_grade', 'math_avg_grade', 'math_max_grade']\n",
    "groupby1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902fce9c-61ba-456f-b9fa-c65b3239cef2",
   "metadata": {},
   "source": [
    "We can group by multiple columns as well. To demonstrate, we'll create a boolean column named `passing_math`, then group by the combination of our new feature, `passing_math`, and the classroom and calculate the average reading grade and the number of individuals in each subgroup. \n",
    "\n",
    "Let's break this problem down and code it step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5772c2ea-6a32-4bfe-8ce1-062880b0bcfa",
   "metadata": {},
   "source": [
    "#### `np.where`\n",
    "\n",
    "First, we can create the new `passing_math` column using a handy NumPy function called `np.where`. It will allow us to base the new column values on whether the values in an existing column, `math`, meet a condition.\n",
    "\n",
    "```python\n",
    "np.where(condition, this_where_True, this_where_False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374c31f2-38a8-4efa-83c1-beda2eee2323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a683346-86a7-47b9-b7b7-cb4c65ce5c35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the new column based on an existing column.\n",
    "\n",
    "df['passing_math'] = np.where(df.math < 70, 'failing', 'passing')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ba6548-a47c-4b09-8e02-dac4567d190b",
   "metadata": {},
   "source": [
    "Next, we will group by the `passing_math` and `classroom` columns and use the `.agg` method to calculate the average reading grade and the number of students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd21507-fad8-499e-bf99-0594191231ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "grade_groups = df.groupby(['passing_math', 'classroom']).reading.agg(['mean', 'count'])\n",
    "grade_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8c8e22-4200-42b0-b8e3-814da7b93a5d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I can even clean up my columns to make my calculations clearer.\n",
    "\n",
    "grade_groups.columns = ['avg_reading_grade', 'count_of_students']\n",
    "grade_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3e3ca5-566c-4ecf-97dc-47d8704239e5",
   "metadata": {},
   "source": [
    "**Takeaways:**\n",
    "\n",
    "We can interpret this output as there being 2 students failing math in classroom A with an average reading grade of 87, 6 students passing math in classroom A with an average reading grade of 87.16, and 4 students passing math in classroom B with an average reading grade of 85.25."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d069ee4-9c39-4a32-b614-d41e7a828d2c",
   "metadata": {},
   "source": [
    "#### `.transform`\n",
    "\n",
    "The `.transform` method can be used to produce a series with the same length of the original dataframe where each value represents the aggregation from the subgroup resulting from the `.groupby`. \n",
    "\n",
    "For example, if we wanted to know the average math grade for each classroom and add this data back to our original dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a90803-05c2-440a-8f16-89f876ebc6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.assign(avg_math_score_by_classroom=df.groupby('classroom').math.transform('mean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782595d7-c67e-4d13-a53c-6d67ef5487da",
   "metadata": {},
   "source": [
    "#### `.describe`\n",
    "\n",
    "Check out what I can do when I combine a `.groupby` with a `.describe`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909f2a0-4314-4230-b799-3fcc4f834767",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('classroom').reading.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e96ac-95c2-4f29-81d7-2556257adbdb",
   "metadata": {},
   "source": [
    "## Merging and Joining\n",
    "\n",
    "Pandas provides several ways to combine dataframes together. We will look at two of them below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925ba8e9-e3e4-48b2-ab7d-72455c446281",
   "metadata": {},
   "source": [
    "### `pd.concat`\n",
    "\n",
    "This function takes in a list or dictionary of Series or DataFrame objects and joins them along a particular axis, row-wise axis=0 or column-wise axis=1.\n",
    "\n",
    "```python\n",
    "# For example, concat with a list of two DataFrames\n",
    "pd.concat([df1, df2], axis=0)\n",
    "```\n",
    "\n",
    "- When your list contains at least one DataFrame, a DataFrame is returned.\n",
    "\n",
    "\n",
    "- When concatenating only Series objects row-wise, axis=0, a Series is returned.\n",
    "\n",
    "\n",
    "- When concatenating Series or DataFrames column-wise, axis=1, a DataFrame is returned.\n",
    "\n",
    "```python\n",
    "# Default is set to row-wise concatenation using an outer join.\n",
    "pd.concat(objs, axis=0, join='outer')\n",
    "```\n",
    "\n",
    "When concatenating dataframes vertically, we basically are just adding more rows to an existing dataframe. In this case, the dataframes we are putting together should have the same column names[^1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872201d-2c0f-4ade-b41d-a00477786ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'a': [1, 2, 3]})\n",
    "df2 = pd.DataFrame({'a': [4, 5, 6]})\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb3b33-dd97-4e80-9f6b-980ad09375f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fde0441-8711-45a0-987d-78ea13be1ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c1cfb4-74e9-4540-9f44-7c90f99ab9e6",
   "metadata": {},
   "source": [
    "**Note** that the indices are preserved on the resulting dataframe; we could set the `ignore_index` parameter to `True` if we wanted these to be sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376bbfe3-9e75-472d-a5d6-cff64229eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df1 = pd.concat([df1, df2], ignore_index=True)\n",
    "concat_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582dcf48-1ae6-4ede-be4b-b339e9d62203",
   "metadata": {},
   "source": [
    "[^1]:\n",
    "    We can concatenate dataframes with different column names, but generally this is not the behavior we want, as pandas will fill in a lot of null values into the resulting dataframe. The exception to this is if the dataframes are aligned on their index (i.e. the labels for each row), then we can provide the `axis=1` keyword argument to `pd.concat` to merge the dataframes horizontally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab9f14a-e916-434b-910c-af41b6dd7f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df2 = pd.DataFrame({'b': [1, 2, 3, 4, 5, 6]})\n",
    "concat_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f294b-a09d-4843-96fc-ecc2ad90e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([concat_df1, concat_df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29633905-a72f-440c-b93a-da79040d1fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([concat_df1, df1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a06709-5534-4515-9fee-887bf8846b72",
   "metadata": {},
   "source": [
    "### `.merge`\n",
    "\n",
    "This method is similar to a SQL join. Here's a [cool read](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html#compare-with-sql-join) making a comparison between the two, if you're interested.\n",
    "\n",
    "```python\n",
    "# df.merge default settings for commonly used parameters.\n",
    "\n",
    "left_df.merge(right_df, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, indicator=False)\n",
    "```\n",
    "\n",
    "How does changing the default argument of the `how` parameter change my resulting DataFrame?\n",
    "\n",
    "`how` == Type of merge to be performed.\n",
    "\n",
    "`how=left`: use only keys from left frame, similar to a SQL left outer join; preserve key order.\n",
    "\n",
    "`how=right`: use only keys from right frame, similar to a SQL right outer join; preserve key order.\n",
    "\n",
    "`how=outer`: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically.\n",
    "\n",
    "`how=inner`: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de39a713-1325-4a15-b470-3d6e80f26a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the users DataFrame.\n",
    "\n",
    "users = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5, 6],\n",
    "    'name': ['bob', 'joe', 'sally', 'adam', 'jane', 'mike'],\n",
    "    'role_id': [1, 2, 3, 3, np.nan, np.nan]\n",
    "})\n",
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff906c76-a17a-4621-84b7-395bbcc95f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the roles DataFrame\n",
    "\n",
    "roles = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4],\n",
    "    'name': ['admin', 'author', 'reviewer', 'commenter']\n",
    "})\n",
    "roles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e3017-fe00-4d49-8862-a202fbd23e46",
   "metadata": {},
   "source": [
    "The `.merge` method will allow us to specify `left_on` and `right_on` to indicate the columns that are the keys used to merge the dataframes together. \n",
    "\n",
    "- In addition, the `how` keyword argument is used to define what type of JOIN we want to do; as we saw above, `inner` is the default setting. \n",
    "\n",
    "- For demonstration purposes, I'm also going to set the `indicator` parameter to `True`, which will create a column indicating whether the merge key appears in the `left_only`, `right_only` or `both` DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396edc15-5cbe-43d3-8ff3-ca300ca05dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an outer join specifying the left and right DataFrame keys.\n",
    "\n",
    "users.merge(roles, left_on='role_id', right_on='id', how='outer', indicator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0cb00c-1e62-406e-b214-954d8e5ba9aa",
   "metadata": {},
   "source": [
    "Notice that we have duplicate column names in the resulting dataframe. By default, pandas will add a suffix of `_x` to any columns in the left dataframe that are duplicated, and `_y` to any columns in the right dataframe that are duplicated. I can clean up my columns if I want to; one way would be to use method chaining, which it demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80476ff1-a6fb-4ace-9a84-2c09adcdb8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = (users.merge(roles, \n",
    "            left_on='role_id', \n",
    "            right_on='id', \n",
    "            how='outer')\n",
    "    .drop(columns='role_id')\n",
    "    .rename(columns={'id_x': 'id', \n",
    "                     'name_x': 'employee',\n",
    "                     'id_y': 'role_id',\n",
    "                     'name_y': 'role'}\n",
    "            )\n",
    ")\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6adff5e-0b9a-443d-a1b4-0e931c1bf77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ef213f-9a4b-4bf1-83a1-e1989cccf79d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
